---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups:
  - extensions
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: monitoring
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: monitoring-vector-configmap
  namespace: monitoring
data:
  vector.toml: |

    # GLOBAL

    [api]
    enabled = true
    address = "0.0.0.0:8686"

    # SOURCES

    [sources.log_file]
    type = "file"
    include = ["/var/log/*.log"]

    [sources.kubernetes_logs]
    type = "kubernetes_logs"
    auto_partial_merge = true
    self_node_name = "${VECTOR_SELF_NODE_NAME}"


    [sources.prometheus_metrics]
    type = "prometheus_scrape"
    endpoints = ["http://monitoring-service.sc:8686/metrics"]


    # SINKS

    [sinks.console]
    inputs = ["log_file"]
    target = "stdout"
    type = "console"
    encoding.codec = "json"

    [sinks.prometheus]
    type = "prometheus_exporter"
    inputs = ["prometheus_metrics"]
    address = "0.0.0.0:9283"
    default_namespace = "sc"

    [sinks.loki]
    type = "loki"
    inputs = ["kubernetes_logs"]
    endpoint = "http://monitoring-service.sc:3100"
    encoding.codec = "json"
    labels = { host = "{{ host }}" }
 
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: loki-config
  namespace: monitoring
data:
  loki-local-config.yaml: |
    auth_enabled: false
    server:
      http_listen_port: 3100
    ingester:
      wal:
        enabled: false
      lifecycler:
        address: 127.0.0.1
        ring:
          kvstore:
            store: inmemory
          replication_factor: 1
        final_sleep: 0s
      chunk_idle_period: 1h       # Any chunk not receiving new logs in this time will be flushed
      max_chunk_age: 1h           # All chunks will be flushed when they hit this age, default is 1h
      chunk_target_size: 1048576  # Loki will attempt to build chunks up to 1.5MB, flushing first if chunk_idle_period or max_chunk_age is reached first
      chunk_retain_period: 30s    # Must be less than or equal to chunk_idle_period
      max_transfer_retries: 0     # Chunk transfers disabled
    schema_config:
      configs:
        - from: 2020-05-15
          store: boltdb-shipper
          object_store: filesystem
          schema: v11
          index:
            prefix: index_
            period: 24h
    storage_config:
      boltdb_shipper:
        active_index_directory: /loki/boltdb-shipper-active
        cache_location: /loki/boltdb-shipper-cache
        cache_ttl: 24h          # Can be increased for faster performance over longer query periods, uses more disk space
        shared_store: filesystem
    compactor:
      working_directory: /loki/boltdb-shipper-compactor
      shared_store: filesystem
    limits_config:
      enforce_metric_name: false
      reject_old_samples: true
      reject_old_samples_max_age: 168h
    chunk_store_config:
      max_look_back_period: 0s
    table_manager:
      retention_deletes_enabled: false
      retention_period: 0s
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval:     15s
      evaluation_interval: 15s
    scrape_configs:
      - job_name: 'vector'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app]
            action: keep
            regex: sidechains-substrate-poc
          - source_labels: [__meta_kubernetes_pod_label_node]
            action: keep
            regex: eve
          - source_labels: [__meta_kubernetes_pod_container_port_number]
            action: keep
            regex: "9182"
      - job_name: 'vector-static'
        static_configs:
          - targets: ['monitoring-service.monitoring:9182', 'alice-service.sc:9182', 'bob-service.sc:9182', 'charlie-service.sc:9182','dave-service.sc:9182','eve-service.sc:9182']
      - job_name: 'kube-state-metrics'
        static_configs:
          - targets: ['kube-state-metrics.monitoring.svc.cluster.local:8080']
      - job_name: 'node-exporter'
        scrape_interval: 5s
        kubernetes_sd_configs:
        - role: node
        relabel_configs:
        - source_labels: [__address__]
          regex: '(.+):(.+)'
          replacement: '${1}:9100'
          target_label: __address__
      - job_name: 'cadvisor'
        scrape_interval: 5s
        kubernetes_sd_configs:
        - role: node
        scheme: https
        tls_config:
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-config
  namespace: monitoring
data:
  grafana.ini: |-
    [auth.anonymous]
    # enable anonymous access
    enabled = true
    # specify organization
    org_name = Main Org.
    # specify role for unauthenticated users
    org_role = Viewer

    [dashboards]
    # The path to the default home dashboard
    default_home_dashboard_path = /etc/grafana/dashboards/default/grafana-dashboard.json
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards-provisioning
  namespace: monitoring
data:
  dashboards.yaml: |-
    apiVersion: 1
    providers:
      - name: 'default'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /etc/grafana/dashboards/default
---
apiVersion: v1
kind: Service
metadata:
  name: monitoring-service
  namespace: monitoring
spec:
  type: NodePort
  selector:
    app: monitoring-pod
  ports:
    - name: grafana
      port: 3000
      targetPort: 3000
      nodePort: 30000
    - name: prometheus
      port: 9090
      targetPort: 9090
      nodePort: 30010
    - name: loki
      port: 3100
      targetPort: 3100
      nodePort: 30020
    - name: vector-api
      port: 8686
      targetPort: 8686
      nodePort: 30030
    - name: vector-prometheus
      port: 9283
      targetPort: 9283
      nodePort: 30040
---
apiVersion: v1
kind: Pod
metadata:
  name: monitoring
  namespace: monitoring
  labels:
    app: monitoring-pod
spec:
  nodeSelector:
    pod: monitoring
  containers:
    - name: vector
      image: timberio/vector:nightly-alpine
      imagePullPolicy: IfNotPresent
      resources:
        limits:
          memory: "4096Mi"
          cpu: "1500m"
        requests:
          memory: "4096Mi"
          cpu: "1500m"
      ports:
        - containerPort: 8686
      env:
        - name: VECTOR_SELF_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
      volumeMounts:
        - name: vector-config
          mountPath: /etc/vector
        - name: vector-data
          mountPath: /var/log
    - name: loki
      image: grafana/loki
      imagePullPolicy: IfNotPresent
      resources:
        limits:
          memory: "4096Mi"
          cpu: "1500m"
        requests:
          memory: "4096Mi"
          cpu: "1500m"
      ports:
        - containerPort: 3100
      args:
        - -config.file=/etc/loki/loki-local-config.yaml
      volumeMounts:
        - name: loki-data
          mountPath: /loki
        - name: loki-config
          mountPath: /etc/loki
    - name: prometheus
      image: prom/prometheus:v2.45.0
      imagePullPolicy: IfNotPresent
      resources:
        limits:
          memory: "4096Mi"
          cpu: "2000m"
        requests:
          memory: "4096Mi"
          cpu: "2000m"
      ports:
        - containerPort: 9090
      volumeMounts:
        - name: prometheus-data
          mountPath: /prometheus
        - name: prometheus-config
          mountPath: /etc/prometheus/prometheus.yml
          subPath: prometheus.yml
    - name: grafana
      image: grafana/grafana:10.0.1
      imagePullPolicy: IfNotPresent
      resources:
        limits:
          memory: "8192Mi"
          cpu: "2000m"
        requests:
          memory: "8192Mi"
          cpu: "2000m"
      ports:
        - containerPort: 3000
      volumeMounts:
        - name: grafana-data
          mountPath: /var/lib/grafana
        - name: grafana-config
          mountPath: /etc/grafana
        - name: grafana-dashboard
          mountPath: /etc/grafana/dashboards/default
        - name: grafana-dashboards-provisioning
          mountPath: /etc/grafana/provisioning/dashboards
  volumes:
    - name: vector-config
      configMap:
        name: monitoring-vector-configmap
    - name: vector-data
      persistentVolumeClaim:
        claimName: monitoring-claim-vector
    - name: loki-data
      persistentVolumeClaim:
        claimName: monitoring-claim-loki
    - name: loki-config
      configMap:
        name: loki-config
    - name: prometheus-data
      persistentVolumeClaim:
        claimName: monitoring-claim-prometheus
    - name: prometheus-config
      configMap:
        name: prometheus-config
    - name: grafana-data
      persistentVolumeClaim:
        claimName: monitoring-claim-grafana
    - name: grafana-config
      configMap:
        name: grafana-config
    - name: grafana-dashboard
      configMap:
        name: grafana-dashboard
    - name: grafana-dashboards-provisioning
      configMap:
        name: grafana-dashboards-provisioning
